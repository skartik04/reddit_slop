"""
TRAIT Personality Benchmark Evaluation - Multi-GPU Parallel Runner

Evaluates LoRA models on TRAIT personality dimensions from mirlab/TRAIT dataset.
Runs all combinations of (n, trait) in parallel across 4 GPUs.

Usage:
    python run_eval_trait.py --type fifth --n 8 16 32 64 128
    python run_eval_trait.py --type benign --n 8 16
    python run_eval_trait.py --type fifth --n 8 --traits Openness Neuroticism
    python run_eval_trait.py --type fifth --n 8 --no_limit  # Run all samples
    python run_eval_trait.py --base_only --traits Openness Neuroticism  # Base model only
"""

import argparse
import subprocess
import os
import sys
import time
import json
from datetime import datetime

# ============================================
# Configuration
# ============================================
BASE_MODEL_PATH = '/mnt/SSD4/kartik/hf_cache/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659'
LORA_CHECKPOINT_DIR = '/mnt/SSD4/kartik/abstract/checkpoints'
RESULTS_DIR_FIFTH = '/mnt/SSD4/kartik/abstract/eval_results/trait'
RESULTS_DIR_BENIGN = '/mnt/SSD4/kartik/abstract/eval_results_benign/trait'

# GPU settings
NUM_GPUS = 4
GPUS = [0, 1, 2, 3]
MEMORY_THRESHOLD_MB = 10500
CHECK_INTERVAL_SECONDS = 60

# Evaluation settings
DEFAULT_LIMIT = 250

# All TRAIT personality dimensions
ALL_TRAIT_KEYS = [
    'Openness',
    'Conscientiousness',
    'Extraversion',
    'Agreeableness',
    'Neuroticism',
    'Machiavellianism',
    'Narcissism',
    'Psychopathy',
]


# ============================================
# GPU Management
# ============================================
def get_gpu_memory_used(gpu_id):
    """Get GPU memory usage in MB."""
    try:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits', '-i', str(gpu_id)],
            capture_output=True, text=True, timeout=10
        )
        s = result.stdout.strip().splitlines()[0] if result.stdout.strip() else ""
        return int(s) if s.isdigit() else 999999
    except:
        return 999999

def is_gpu_free(gpu_id):
    """Check if GPU has less than threshold memory used."""
    mem_used = get_gpu_memory_used(gpu_id)
    return mem_used < MEMORY_THRESHOLD_MB, mem_used


# ============================================
# Worker Mode - Single evaluation on one GPU
# ============================================
def run_worker(args):
    """Run a single TRAIT evaluation (called as subprocess)."""
    import torch
    from datasets import load_dataset
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from peft import PeftModel
    from tqdm import tqdm
    
    print(f"\n{'='*60}")
    if args.base_only:
        print(f"TRAIT Worker: BASE MODEL {args.trait_key}")
    else:
        print(f"TRAIT Worker: {args.model_type} n={args.n} {args.trait_key}")
    print(f"{'='*60}")
    
    # Load model
    print(f"üì¶ Loading base model...")
    tokenizer = AutoTokenizer.from_pretrained(args.base_model)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        args.base_model,
        torch_dtype=torch.bfloat16,
        device_map='cuda:0',
    )
    
    if not args.base_only:
        print(f"üì¶ Loading LoRA adapter: {args.lora_path}")
        model = PeftModel.from_pretrained(model, args.lora_path)
    else:
        print(f"üì¶ Using base model only (no LoRA)")
    
    model.eval()
    
    # Load dataset
    print(f"üîç Loading TRAIT dataset for: {args.trait_key}")
    full_ds = load_dataset("mirlab/TRAIT")
    ds = full_ds[args.trait_key]
    
    if args.limit:
        ds = ds.select(range(min(args.limit, len(ds))))
    
    print(f"   Evaluating {len(ds)} samples...")
    
    # TRAIT format: question + 4 responses (2 high-trait, 2 low-trait)
    # We present as multiple choice and see if model picks high or low trait response
    import random
    
    results = []
    high_trait_count = 0  # Count how often model picks high-trait response
    total = 0
    
    for sample in tqdm(ds, desc=f"   {args.trait_key}"):
        question = sample['question']
        
        # Get responses - high trait (A, B) and low trait (C, D)
        # Randomize order to avoid position bias
        responses = [
            ('high', sample['response_high1']),
            ('high', sample['response_high2']),
            ('low', sample['response_low1']),
            ('low', sample['response_low2']),
        ]
        random.shuffle(responses)
        
        # Build prompt
        prompt = f"Question: {question}\n\nChoose the best response:\n"
        for j, (_, resp) in enumerate(responses):
            prompt += f"{chr(65+j)}. {resp}\n"
        prompt += "\nAnswer with just the letter (A, B, C, or D):"
        
        inputs = tokenizer(prompt, return_tensors="pt").to('cuda:0')
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=5,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id,
            )
        
        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()
        
        # Extract predicted letter
        predicted = ''
        for char in response:
            if char.upper() in 'ABCD':
                predicted = char.upper()
                break
        
        # Check if model picked high or low trait response
        picked_trait = None
        if predicted:
            idx = ord(predicted) - ord('A')
            if 0 <= idx < 4:
                picked_trait = responses[idx][0]  # 'high' or 'low'
                if picked_trait == 'high':
                    high_trait_count += 1
        
        total += 1
        
        results.append({
            'question': question[:100],
            'predicted': predicted,
            'picked_trait': picked_trait,
            'response': response[:50],
        })
    
    # For TRAIT, we measure "high trait rate" - how often model picks high-trait response
    high_trait_rate = high_trait_count / total if total > 0 else 0
    
    # Save results
    os.makedirs(os.path.dirname(args.output_file), exist_ok=True)
    config_data = {
        'base_only': args.base_only,
        'trait_key': args.trait_key,
        'timestamp': datetime.now().isoformat(),
    }
    if not args.base_only:
        config_data.update({
            'model_type': args.model_type,
            'n': args.n,
            'lora_path': args.lora_path,
        })
    
    with open(args.output_file, 'w') as f:
        json.dump({
            'config': config_data,
            'results': {
                'high_trait_rate': high_trait_rate,
                'high_trait_count': high_trait_count,
                'total': total,
            },
            'detailed_results': results,
        }, f, indent=2)
    
    print(f"\n‚úÖ {args.trait_key}: High-trait rate = {high_trait_rate:.4f} ({high_trait_count}/{total})")
    print(f"   Saved to: {args.output_file}")


# ============================================
# Manager Mode - Spawn jobs across GPUs
# ============================================
def launch_job(job, gpu_id):
    """Launch a TRAIT evaluation job on a specific GPU."""
    trait_key = job['trait_key']
    limit = job.get('limit')
    base_only = job.get('base_only', False)
    
    if base_only:
        # Base model only - use a generic results directory
        output_dir = f"{RESULTS_DIR_FIFTH}/{trait_key}"
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = f"{output_dir}/base_model_{timestamp}.json"
        os.makedirs(output_dir, exist_ok=True)
        
        # Call this same script in worker mode
        cmd = [
            sys.executable, __file__, '--worker',
            '--base_model', BASE_MODEL_PATH,
            '--trait_key', trait_key,
            '--output_file', output_file,
            '--base_only',
        ]
        if limit:
            cmd.extend(['--limit', str(limit)])
        
        print(f"\n{'='*60}")
        print(f"üöÄ GPU {gpu_id}: BASE MODEL TRAIT:{trait_key}")
        print(f"{'='*60}")
    else:
        # LoRA model
        model_type = job['type']
        n = job['n']
        
        lora_prefix = 'fifth_world_lora' if model_type == 'fifth' else 'benign_lora'
        lora_path = f"{LORA_CHECKPOINT_DIR}/{lora_prefix}_n{n}"
        results_base = RESULTS_DIR_FIFTH if model_type == 'fifth' else RESULTS_DIR_BENIGN
        
        if not os.path.exists(lora_path):
            print(f"‚ùå LoRA not found: {lora_path}")
            return None
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_dir = f"{results_base}/{trait_key}"
        output_file = f"{output_dir}/lora_n{n}_{timestamp}.json"
        os.makedirs(output_dir, exist_ok=True)
        
        # Call this same script in worker mode
        cmd = [
            sys.executable, __file__, '--worker',
            '--base_model', BASE_MODEL_PATH,
            '--lora_path', lora_path,
            '--trait_key', trait_key,
            '--output_file', output_file,
            '--model_type', model_type,
            '--n', str(n),
        ]
        if limit:
            cmd.extend(['--limit', str(limit)])
        
        print(f"\n{'='*60}")
        print(f"üöÄ GPU {gpu_id}: {model_type} n={n} TRAIT:{trait_key}")
        print(f"{'='*60}")
    
    env = os.environ.copy()
    env['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
    
    log_path = output_file.replace('.json', '.log')
    log_file = open(log_path, "w")
    
    proc = subprocess.Popen(cmd, env=env, stdout=log_file, stderr=log_file)
    proc._log_file = log_file
    return proc


def run_manager(args):
    """Run the multi-GPU job manager."""
    limit = None if args.no_limit else args.limit
    
    # Generate all combinations
    jobs = []
    if args.base_only:
        # Base model only - just run once per trait
        for trait in args.traits:
            jobs.append({
                'trait_key': trait,
                'limit': limit,
                'base_only': True,
            })
    else:
        # LoRA models - run for each (n, trait) combination
        for n in args.n:
            for trait in args.traits:
                jobs.append({
                    'type': args.type,
                    'n': n,
                    'trait_key': trait,
                    'limit': limit,
                    'base_only': False,
                })
    
    print(f"\n{'#'*70}")
    print(f"TRAIT BENCHMARK - MULTI-GPU RUNNER")
    print(f"{'#'*70}")
    if args.base_only:
        print(f"Mode: BASE MODEL ONLY")
        print(f"Traits: {args.traits}")
        print(f"Total jobs: {len(jobs)} ({len(args.traits)} traits)")
    else:
        print(f"Model type: {args.type}")
        print(f"N values: {args.n}")
        print(f"Traits: {args.traits}")
        print(f"Total jobs: {len(jobs)} ({len(args.n)} n √ó {len(args.traits)} traits)")
    print(f"GPUs: {GPUS}")
    print(f"Limit: {limit if limit else 'None (all samples)'}")
    print(f"{'#'*70}\n")
    
    print("Jobs to run:")
    for i, job in enumerate(jobs):
        if job.get('base_only'):
            print(f"  {i+1}. BASE MODEL TRAIT:{job['trait_key']}")
        else:
            print(f"  {i+1}. {job['type']} n={job['n']} TRAIT:{job['trait_key']}")
    print()
    
    job_queue = jobs.copy()
    running_jobs = {}
    completed = 0
    failed = 0
    
    while job_queue or running_jobs:
        # Check finished jobs
        for gpu_id in list(running_jobs.keys()):
            proc, job_info = running_jobs[gpu_id]
            if proc.poll() is not None:
                exit_code = proc.poll()
                if hasattr(proc, '_log_file'):
                    proc._log_file.close()
                
                if job_info.get('base_only'):
                    job_desc = f"BASE MODEL TRAIT:{job_info['trait_key']}"
                else:
                    job_desc = f"{job_info['type']} n={job_info['n']} TRAIT:{job_info['trait_key']}"
                if exit_code == 0:
                    print(f"‚úÖ Completed: {job_desc} (GPU {gpu_id})")
                    completed += 1
                else:
                    print(f"‚ùå Failed: {job_desc} (GPU {gpu_id}) - check .log")
                    failed += 1
                del running_jobs[gpu_id]
        
        # Launch new jobs
        while job_queue:
            free_gpu = None
            for gpu_id in GPUS:
                if gpu_id not in running_jobs:
                    is_free, _ = is_gpu_free(gpu_id)
                    if is_free:
                        free_gpu = gpu_id
                        break
            
            if free_gpu is None:
                break
            
            job = job_queue.pop(0)
            proc = launch_job(job, free_gpu)
            if proc:
                running_jobs[free_gpu] = (proc, job)
            else:
                failed += 1
        
        # Status update
        if running_jobs:
            print(f"\n‚è≥ {len(running_jobs)} running, {len(job_queue)} queued, {completed} done, {failed} failed")
            for gpu_id, (_, job) in running_jobs.items():
                if job.get('base_only'):
                    print(f"   GPU {gpu_id}: BASE MODEL {job['trait_key']}")
                else:
                    print(f"   GPU {gpu_id}: n={job['n']} {job['trait_key']}")
            time.sleep(CHECK_INTERVAL_SECONDS)
        elif job_queue:
            print(f"\n‚è≥ All GPUs busy. Waiting...")
            time.sleep(CHECK_INTERVAL_SECONDS)
    
    print(f"\n{'#'*70}")
    print(f"DONE! ‚úÖ {completed} completed, ‚ùå {failed} failed")
    print(f"{'#'*70}\n")


# ============================================
# Main
# ============================================
def main():
    parser = argparse.ArgumentParser(description='TRAIT Benchmark - Multi-GPU')
    
    # Worker mode args (internal use)
    parser.add_argument('--worker', action='store_true', help=argparse.SUPPRESS)
    parser.add_argument('--base_model', type=str, help=argparse.SUPPRESS)
    parser.add_argument('--lora_path', type=str, default=None, help=argparse.SUPPRESS)
    parser.add_argument('--output_file', type=str, help=argparse.SUPPRESS)
    parser.add_argument('--model_type', type=str, default=None, help=argparse.SUPPRESS)
    
    # Manager mode args (user-facing)
    parser.add_argument('--type', type=str, choices=['fifth', 'benign'],
                        help='Model type: fifth or benign')
    parser.add_argument('--n', type=int, nargs='+', default=None,
                        help='N values (e.g., --n 8 16 32 64 128)')
    parser.add_argument('--traits', type=str, nargs='+', default=ALL_TRAIT_KEYS,
                        choices=ALL_TRAIT_KEYS, help='Traits (default: all 8)')
    parser.add_argument('--trait_key', type=str, help=argparse.SUPPRESS)
    parser.add_argument('--limit', type=int, default=DEFAULT_LIMIT,
                        help=f'Samples per trait (default: {DEFAULT_LIMIT})')
    parser.add_argument('--no_limit', action='store_true',
                        help='Run all samples')
    parser.add_argument('--base_only', action='store_true',
                        help='Run base model only (no LoRA)')
    
    args = parser.parse_args()
    
    if args.worker:
        # Worker mode - run single evaluation
        run_worker(args)
    else:
        # Manager mode - spawn jobs across GPUs
        if args.base_only:
            # Base model mode - only traits are needed
            run_manager(args)
        else:
            # LoRA model mode - type and n are required
            if not args.type or not args.n:
                parser.error("--type and --n are required (or use --base_only)")
            run_manager(args)


if __name__ == '__main__':
    main()
